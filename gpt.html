<!DOCTYPE HTML>
<!--
	Opeyemi Abodunrin's Portfolio
-->
<html>
	<head>
		<title>Text Gen</title>
		<meta charset="utf-8" />
		<meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no" />
		<link rel="stylesheet" href="assets/css/main.css" />
		<noscript><link rel="stylesheet" href="assets/css/noscript.css" /></noscript>
	</head>
	<body class="is-preload">

		<!-- Wrapper -->
			<div id="wrapper">

				<!-- Header -->
					<header id="header">
						<a href="gpt.html" class="logo">Text Generation using GPT</a>
					</header>

				<!-- Nav -->
					<nav id="nav">
						<ul class="links">
							<li><a href="index.html">Home</a></li>
							
						</ul>
						<ul class="icons">
							<li><a href="#" class="icon brands fa-twitter"><span class="label">Twitter</span></a></li>
							<li><a href="#" class="icon brands fa-facebook-f"><span class="label">Facebook</span></a></li>
							<li><a href="#" class="icon brands fa-instagram"><span class="label">Instagram</span></a></li>
							<li><a href="#" class="icon brands fa-github"><span class="label">GitHub</span></a></li>
						</ul>
					</nav>

				<!-- Main -->
					<div id="main">

						<!-- Post -->
							<section class="post">
								<header class="major">
									<!--span class="date">April 25, 2017</span-->
									<h1>Text Generation using GPT<br />
                                        </h1>
									<p>Open AI ChatGPT, Amazon Titan, Google Gemini, and Microsoft Co-pilot utilise NLP Algorithms to decipher text, allowing users to interact with devices using Natural Language. 
										All these were possible because of the propelling force behind NLP, Artificial Intelligence (AI) using Neural Networks to mimic the human brain.</p>
                                    
								</header>
								<div class="image main"><a href="https://github.com/deCrystals/TextGeneration/blob/main/gpt2.py"><img src="images/gpt2.png" alt="" /></a></div>
								<p style="margin-bottom: 10px"><strong>Background</strong></p>
								<p style="margin-bottom: 10px">Text Generation is a subfield of Natural Language Processing (NLP) dedicated to developing software systems that can produce coherent and comprehensible text. 
									This field is crucial in NLP and strives to create realistic and indistinguishable text from human-written content.
                                    One approach that has made a significant impact in the field of natural language processing (NLP) is the Generative Pre-Trained Transformer (GPT), which was introduced in 2018.<br /> 
									Its brilliance lies in its unique semi-supervised training on a massive text corpus. 
									The model architecture, built on a variant of the Transformer, uses a multi-layered decoder to better handle long-term dependencies in text. This results in the model capturing richer 
									linguistic structure and performing well across diverse NLP tasks.</p>
								<p>For this project, I harnessed the groundbreaking capabilities of GPT-2, a true marvel of innovation in the field of NLP. Its extensive pre-training on a colossal dataset, a staggering 40GB 
									corpus encompassing millions of documents across varied sources, sets it apart from its predecessor, GPT -1. The architecture, a 12-level, 12-headed transformer decoder, 
									has been enhanced with normalisation and a significant parameter increase (1.5 billion). However, this immense training necessitates substantial computational resources.</p>
                                <p style="margin-bottom: 10px"><strong>Data Collection</strong></p>
								 <p style="margin-bottom: 10px">The success of text generation depends significantly on the quality and relevance of the data used for training. 
									It involves gathering substantial text aligned with the desired output to provide a foundation for coherent and contextually accurate text generation.
									In this project, after extracting the BBC News dataset, Microsoft Excel was used to process and organise the text data, specifically focusing on the General News section. 
									This curated, factual dataset was then used to fine-tune a GPT-2 model, equipping it to generate contextually relevant and well-informed content.
									</p>
								<p style="margin-bottom: 10px"><strong> Data Preprocessing</strong></p>
									<p>Data preprocessing is essential in preparing text for use in text generation models. 
										It involves cleaning the raw text data to make it compatible with the model for training. Text in its natural 
									form contains irrelevant characters, punctuation, and inconsistencies that can hinder the model's ability to learn effectively. 
									Preprocessing tackles these issues by removing unnecessary symbols and emojis, 
									eliminating duplicate entries to focus on unique patterns, and converting the text to lowercase for consistency. 
									Additionally, some preprocessing steps normalise the text by stemming or lemmatising words, which
									 reduces them to their base form. These cleaning techniques significantly improve the data quality the model trains on, 
									 ultimately leading to better text generation. Used Pythonâ€™s PyTorch libraries 
									 for data preprocessing. </p>
                                <p style="margin-bottom: 10px"><strong>Training/ fine-tuning the model</strong></p> 
								<p style="margin-bottom: 10px">Training, also referred to as fine-tuning, is where the magic happens! We guide the GPT-2 model to learn the intricacies of text generation. 
									This involves feeding the model our prepared text data (dataset) and adjusting various settings to optimise its learning process. 
									Some crucial parameters include the batch size (the amount of data processed at once) and the number of training epochs (repetitions). 
									An optimiser (AdamW, in this case) helps the model adjust its internal parameters to produce better outputs as it trains.</p>
                                <p style="margin-bottom: 10px"><strong>Validation and Evaluation</strong>
								<p style="margin-bottom: 10px">As a diligent student must be tested, the model requires thorough evaluation to ensure it performs effectively on new, unseen data. 
									This phase, known as validation and evaluation, is essential for assessing the model's ability to generate high-quality, human-like text. 
									A new dataset was used to test its capabilities rigorously, which is data the model has not previously encountered.
									Metrics like ROUGE and BLEU help quantify the model's performance by measuring the similarity between the generated text and human-written text. 
									Furthermore, human evaluation is invaluable for assessing aspects like coherence, fluency, and overall quality that automated metrics may overlook. 
									This comprehensive testing process precisely measures the model's effectiveness and reliability.
									Rigorous testing is critical to ensuring high standards. The trained model undergoes a meticulous assessment with independent datasets,
									 with metrics such as ROUGE, BLEU, and human judgment employed to evaluate text quality and coherence, ultimately instilling confidence in the model's dependability.
									 </p>
                                <p>"The complete code for this project is available
									<ul class="actions special">
									<li><a href="https://github.com/deCrystals/TextGeneration/blob/main/gpt2.py" class="button large">View Project code</a></li>
								</ul> </p>

					</div>

				<!-- Footer --
					<footer id="footer">
						<section>
							<form method="post" action="#">
								<div class="fields">
									<div class="field">
										<label for="name">Name</label>
										<input type="text" name="name" id="name" />
									</div>
									<div class="field">
										<label for="email">Email</label>
										<input type="text" name="email" id="email" />
									</div>
									<div class="field">
										<label for="message">Message</label>
										<textarea name="message" id="message" rows="3"></textarea>
									</div>
								</div>
								<ul class="actions">
									<li><input type="submit" value="Send Message" /></li>
								</ul>
							</form>
						</section>
						<section class="split contact">
							<section class="alt">
								<h3>Address</h3>
								<p>1234 Somewhere Road #87257<br />
								Nashville, TN 00000-0000</p>
							</section>
							<section>
								<h3>Phone</h3>
								<p><a href="#">(000) 000-0000</a></p>
							</section>
							<section>
								<h3>Email</h3>
								<p><a href="#">info@untitled.tld</a></p>
							</section>
							<section>
								<h3>Social</h3>
								<ul class="icons alt">
									<li><a href="#" class="icon brands alt fa-twitter"><span class="label">Twitter</span></a></li>
									<li><a href="#" class="icon brands alt fa-facebook-f"><span class="label">Facebook</span></a></li>
									<li><a href="#" class="icon brands alt fa-instagram"><span class="label">Instagram</span></a></li>
									<li><a href="#" class="icon brands alt fa-github"><span class="label">GitHub</span></a></li>
								</ul>
							</section>
						</section>
					</footer>-->

				<!-- Copyright -->
					<div id="copyright">
						<ul><li>&copy; Untitled</li><li>Design: DeCrystals</a></li></ul>
					</div>

			</div>

		<!-- Scripts -->
			<script src="assets/js/jquery.min.js"></script>
			<script src="assets/js/jquery.scrollex.min.js"></script>
			<script src="assets/js/jquery.scrolly.min.js"></script>
			<script src="assets/js/browser.min.js"></script>
			<script src="assets/js/breakpoints.min.js"></script>
			<script src="assets/js/util.js"></script>
			<script src="assets/js/main.js"></script>

	</body>
</html>