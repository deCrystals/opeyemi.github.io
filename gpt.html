<!DOCTYPE HTML>
<!--
	Opeyemi Abodunrin's Portfolio
-->
<html>
	<head>
		<title>Text Gen</title>
		<meta charset="utf-8" />
		<meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no" />
		<link rel="stylesheet" href="assets/css/main.css" />
		<noscript><link rel="stylesheet" href="assets/css/noscript.css" /></noscript>
	</head>
	<body class="is-preload">

		<!-- Wrapper -->
			<div id="wrapper">

				<!-- Header -->
					<header id="header">
						<a href="gpt.html" class="logo">Text Generation using GPT</a>
					</header>

				<!-- Nav -->
					<nav id="nav">
						<ul class="links">
							<li><a href="index.html">Home</a></li>
							
						</ul>
						<ul class="icons">
							<li><a href="#" class="icon brands fa-twitter"><span class="label">Twitter</span></a></li>
							<li><a href="#" class="icon brands fa-facebook-f"><span class="label">Facebook</span></a></li>
							<li><a href="#" class="icon brands fa-instagram"><span class="label">Instagram</span></a></li>
							<li><a href="#" class="icon brands fa-github"><span class="label">GitHub</span></a></li>
						</ul>
					</nav>

				<!-- Main -->
					<div id="main">

						<!-- Post -->
							<section class="post">
								<header class="major">
									<!--span class="date">April 25, 2017</span-->
									<h1>Text Generation using GPT<br />
                                        Using (GPT)</h1>
									<p></p>
                                    <p>Text Generation is a subfield of Natural Language Processing (NLP) dedicated to developing software systems that can produce coherent and comprehensible text. This field is crucial in NLP and strives to create realistic and indistinguishable text from human-written content.
                                    One approach that has made a significant impact in the field of natural language processing (NLP) is the Generative Pre-Trained Transformer (GPT), which was introduced in 2018.<br /> Its brilliance lies in its unique semi-supervised training on a massive text corpus. The model architecture, built on a variant of the Transformer, uses a multi-layered decoder to better handle long-term dependencies in text. This results in the model capturing richer linguistic structure and performing well across diverse NLP tasks.</p>
								</header>
								<div class="image main"> <a href="https://github.com/deCrystals/TextGeneration/blob/main/gpt2.py"><img src="images/gpt2.png" alt="" /></a></div>
								<p>For this project, I harnessed the groundbreaking capabilities of GPT-2, a true marvel of innovation in the field of NLP. Its extensive pre-training on a colossal dataset, a staggering 40GB corpus encompassing millions of documents across varied sources, sets it apart from its predecessor, GPT -1. The architecture, a 12-level, 12-headed transformer decoder, has been enhanced with normalisation and a significant parameter increase (1.5 billion). However, this immense training necessitates substantial computational resources.</p>
                                <p>The project used Python, the Hugging Face Transformer library, and PyTorch for implementation. 
                                    .</p>
								<p>Data Collection: Gathering relevant textual data is not just a step but a critical foundation in generating text. 
                                    It involves gathering texts pertinent to the subject matter, which form the bedrock for the AI system to learn from and produce coherent and contextually 
                                    appropriate text. It is an essential and invaluable step in the text-generation process.</p>
                                <p>Data preprocessing is a crucial step in generating text. It involves cleaning the data and making it compatible with the model for further analysis. Raw text cannot be used in text generation models; hence, preprocessing is needed. This stage includes removing irrelevant characters like special symbols or emojis, eliminating duplicate entries to avoid redundancy, correcting spelling or grammatical mistakes, and converting the text to lowercase for normalisation. These steps significantly enhance the overall quality of the generated text. Tokenisation is a crucial part of this process, where the data (input text) is divided into tokens, words, sub-words, or symbols (such as punctuation) and an integer is assigned to each token.</p>
                                <p>Training/ fine-tuning the model: The training model function comprises several parameters, including the dataset, model (GPT2), tokeniser, batch size (default is 16), training epochs, optimiser (AdamW), maximum sequence length for input data, and the number of warm-up steps for the learning rate scheduler. Gradients are accumulated, and an optimiser update is executed every 100 steps. During training, the model is configured to use the GPU for enhanced efficiency with "device = torch.device("cuda")". Additionally, the input tensor is allocated to the GPU. The model is set to training mode with "model.train()"; the loss is monitored throughout the training process.</p>
                                <p>Validation and Evaluation: The importance of rigorous testing cannot be overstated. The trained model undergoes meticulous scrutiny with independent datasets, ensuring its performance meets the highest standards. Metrics like ROGUE, BLEU, and human judgment are employed to assess the quality and coherence of the generated text, instilling confidence in the model's reliability. </p>
                                <p>"The complete code for this project is available <a href="https://github.com/deCrystals/TextGeneration/blob/main/gpt2.py">here</a>"</p>

					</div>

				<!-- Footer --
					<footer id="footer">
						<section>
							<form method="post" action="#">
								<div class="fields">
									<div class="field">
										<label for="name">Name</label>
										<input type="text" name="name" id="name" />
									</div>
									<div class="field">
										<label for="email">Email</label>
										<input type="text" name="email" id="email" />
									</div>
									<div class="field">
										<label for="message">Message</label>
										<textarea name="message" id="message" rows="3"></textarea>
									</div>
								</div>
								<ul class="actions">
									<li><input type="submit" value="Send Message" /></li>
								</ul>
							</form>
						</section>
						<section class="split contact">
							<section class="alt">
								<h3>Address</h3>
								<p>1234 Somewhere Road #87257<br />
								Nashville, TN 00000-0000</p>
							</section>
							<section>
								<h3>Phone</h3>
								<p><a href="#">(000) 000-0000</a></p>
							</section>
							<section>
								<h3>Email</h3>
								<p><a href="#">info@untitled.tld</a></p>
							</section>
							<section>
								<h3>Social</h3>
								<ul class="icons alt">
									<li><a href="#" class="icon brands alt fa-twitter"><span class="label">Twitter</span></a></li>
									<li><a href="#" class="icon brands alt fa-facebook-f"><span class="label">Facebook</span></a></li>
									<li><a href="#" class="icon brands alt fa-instagram"><span class="label">Instagram</span></a></li>
									<li><a href="#" class="icon brands alt fa-github"><span class="label">GitHub</span></a></li>
								</ul>
							</section>
						</section>
					</footer>-->

				<!-- Copyright -->
					<div id="copyright">
						<ul><li>&copy; Untitled</li><li>Design: DeCrystals</a></li></ul>
					</div>

			</div>

		<!-- Scripts -->
			<script src="assets/js/jquery.min.js"></script>
			<script src="assets/js/jquery.scrollex.min.js"></script>
			<script src="assets/js/jquery.scrolly.min.js"></script>
			<script src="assets/js/browser.min.js"></script>
			<script src="assets/js/breakpoints.min.js"></script>
			<script src="assets/js/util.js"></script>
			<script src="assets/js/main.js"></script>

	</body>
</html>